# 课题 7：简化 Bandit/RL 风格的动态定价策略学习（进阶）

> 提示：本课题为进阶选题，适合对强化学习 / 多臂老虎机感兴趣且编程能力较强的同学。  
> 通用规范（EDA、Train/Val/Test、Git、报告撰写等）请参考课程总 README。

---

## 一、课题背景与核心问题

在真实平台上做动态定价实验需要在线 A/B 测试，成本高且风险大。  
本课题尝试基于历史的“价格–收益”数据，构造一个**简化的离线模拟环境**，  
在这个环境中，用多臂老虎机（Multi-armed Bandit）方法学习一套定价策略。

核心问题：

- 在同质房源子集上，如何将不同价格档视作“拉臂动作”？  
- 不同 Bandit 算法（ε-greedy / UCB / Thompson Sampling 等）在这个环境中表现如何？  
- 在模拟的长期交互中，收敛速度和最终收益有何差异？

---

## 二、数据字段建议

### 1. Calendar Rates（构造环境用）

- 价格与表现：  
  `rate_avg, booked_rate_avg, occupancy, revenue`（按月）

### 2. Listings（用于筛选房源）

- 用于挑选某一类相对同质的房源（相似房型/位置），如：  
  `city, room_type, guests, superhost, professional_management` 等。

---

## 三、必做任务

### 1. 环境抽象：历史数据 → 模拟环境

以某个城市 + 子市场为例（例如“整套房 + 2–4 人”），完成：

1. 选定一批“相对同质”的房源子集；  
2. 将历史 `rate_avg` 按区间离散成若干“价格档”（arms），例如：
   - 档 1：0–300；档 2：300–500；档 3：500–700；……（示例）；
3. 对每个价格档，估计一个“收益分布”，例如：
   - 计算该档下的平均 `revenue` 或平均 `RevPAR`，以及方差；
4. 抽象出一个**静态的 Bandit 环境**：
   - 每当算法选择某个价格档，即从该档对应的经验分布中采样一个 reward（可加少量噪声）。

> 要求在报告中明确写清：  
> - 环境是基于 **历史统计** 构造的“玩具模型”；  
> - 与真实线上环境存在差距，仅用于教学与算法对比。

### 2. 多臂老虎机算法实现

至少实现 **两种经典 Bandit 算法**：

1. 必做：ε-greedy  
   - 需要设置探索率 ε（可尝试不同 ε 值比较效果）；
2. 下列算法中任选 **一种**：
   - UCB（Upper Confidence Bound）；  
   - 或 Thompson Sampling。

在构造好的环境中模拟多轮“定价实验”流程：

- 初始时对各价格档的认知很少；  
- 每一轮：
  1. 根据当前策略选一个价格档（arm）；  
  2. 环境根据该档的经验分布返回一个 reward；  
  3. 更新该档的统计信息，迭代策略参数。

### 3. 效果评估与对比

在模拟中记录并绘制（对每种算法分别）：

- 随时间轮数变化的 **累积 reward 曲线**；  
- 随时间轮数变化的 **regret 曲线**：  
  - 相对“总是选择历史上平均 reward 最高的价格档”的损失。

比较：

- 不同算法的收敛速度（多久开始稳定选择“较优价格档”）；  
- 长期累积收益谁更高。

在报告中给出解释与分析。

---

## 四、选做任务（加分项）

1. **非平稳环境（需求变化）**  
   - 让“价格–收益”关系随时间缓慢变化，例如：  
     - 前半段某价格档很好，后半段由于需求变化变差；  
   - 重新运行 Bandit 算法，观察不同算法在非平稳环境下的表现差异（是否能适应变化）。

2. **多环境对比**  
   - 从 Calendar 数据中选择多个特征不同的房源子集（如高价区 vs 低价区）；  
   - 为每个子集构造一个独立的 Bandit 环境；  
   - 比较各环境中学到的策略是否不同，总结：不同市场对定价策略的敏感度是否一致。

3. **与简单“固定价格策略”的对比**  
   - 以“始终选择固定价格档”的策略作为 baseline；  
   - 比较 Bandit 策略相对 baseline 的提升幅度。
